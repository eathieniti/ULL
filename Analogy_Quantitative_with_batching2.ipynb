{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing  # Two samples, with 3 dimensions.\n",
    "import analyze\n",
    "import evaluate\n",
    "import imp\n",
    "import time\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy \n",
    "\n",
    "path = './data/'\n",
    "\n",
    "embedding_names = ['bow2','bow5', 'deps']\n",
    "#embedding_names = ['bow2', 'bow5']\n",
    "\n",
    "#\n",
    "# embeddings: list of dicts holding the data for each embedding\n",
    "# dict: {name - embedding name,\n",
    "#       filename - embedding data filename\n",
    "#       words - dict holding the words  and their vectors\n",
    "#    \n",
    "embeddings = []\n",
    "\n",
    "\n",
    "for name in embedding_names:\n",
    "    # words: dict, holds all words and their vectors for each embedding\n",
    "    words = {}\n",
    "    words_normalized = {}\n",
    "    filename = name + '.words.bz2'\n",
    "    embedding_df = pd.read_table(path + filename , sep=' ', header=None)\n",
    "    for index, row in embedding_df.iterrows():\n",
    "        words[row[0]] = row[1:]\n",
    "    \n",
    "    \n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "    embeddings.append({'name':name, 'filename': filename, \n",
    "                            'words': words, ' embedding_vectors':  embedding_vectors,\n",
    "                       'embedding_vocabulary': embedding_vocabulary})\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183870\n",
      "183870\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    print(len(embedding['embedding_vocabulary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to calculate MRR\n",
    "def accuracy_and_MRR(X, Y, indices, exclude):\n",
    "    # \n",
    "    # X: query vectors matrix (size: A x M)\n",
    "    # Y: embedding vectors matrix (size: Nwords x Mfeatures)\n",
    "    # indices: indices of the real (size: A)\n",
    "    # exclude = list of indices to exclude for each analogy word\n",
    "    # Compute distances between each codeword and each other codeword\n",
    "    distance_matrix = scipy.spatial.distance.cdist(X, Y, metric='cosine')\n",
    "    exclude = np.array(exclude)\n",
    "    m = np.amax(distance_matrix)\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        #print(exclude[:,0].flatten())\n",
    "        distance_matrix[np.arange(X.shape[0]), exclude[:,i].flatten()] =  m\n",
    "    # Rank is the number of distances smaller than the correct distance, a\n",
    "    # specified by the indices arg\n",
    "    \n",
    "    n_le = distance_matrix.T <= distance_matrix[np.arange(X.shape[0]), indices]\n",
    "    answer_indexes = np.argsort(distance_matrix)[:,0]\n",
    "    possible_answer_indexes = np.argsort(distance_matrix)[:,1:4]\n",
    "    matches = np.array(indices - answer_indexes)\n",
    "    acc = sum(matches == 0)/len(matches)\n",
    "    n_lt = distance_matrix.T < distance_matrix[np.arange(X.shape[0]), indices]\n",
    "    \n",
    "    return (np.mean(1./n_le.sum(axis=0)),acc , answer_indexes, possible_answer_indexes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load in the analogy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Athens</th>\n",
       "      <th>Greece</th>\n",
       "      <th>Baghdad</th>\n",
       "      <th>Iraq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Athens  Greece  Baghdad         Iraq\n",
       "0  Athens  Greece  Bangkok     Thailand\n",
       "1  Athens  Greece  Beijing        China\n",
       "2  Athens  Greece   Berlin      Germany\n",
       "3  Athens  Greece     Bern  Switzerland\n",
       "4  Athens  Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analogy Task\n",
    "\n",
    "analogy_file = 'questions-words.txt'\n",
    "analogy_words_all = pd.read_table(path + analogy_file , sep=' ', header=1)\n",
    "analogy_words_all.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute analogy word and compare to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow2\n",
      "Calculating batch, size: 0\n",
      "0.0003077983856201172 seconds\n",
      "Calculating batch, size: 10001\n",
      "0.444614031333 0.371708511941 1633\n",
      "205.90572595596313 seconds\n",
      "Calculating batch, size: 6000\n",
      "0.744238190823 0.663048498845 4330\n",
      "576.236487865448 seconds\n",
      "Calculating batch, size: 3555\n",
      "0.763610932179 0.686462144666 3553\n",
      "446.7125039100647 seconds\n",
      "bow5\n",
      "Calculating batch, size: 0\n",
      "0.08013510704040527 seconds\n",
      "Calculating batch, size: 10001\n",
      "0.466912463141 0.380281690141 1633\n",
      "196.434720993042 seconds\n",
      "Calculating batch, size: 6000\n",
      "0.734695955207 0.641801385681 4330\n",
      "589.0359320640564 seconds\n",
      "Calculating batch, size: 3555\n",
      "0.738747495437 0.640866873065 3553\n",
      "521.11399102211 seconds\n",
      "521.1686689853668 seconds\n"
     ]
    }
   ],
   "source": [
    "start =  time.time()\n",
    "\n",
    "all_bstars = []\n",
    "\n",
    "analogy_1 = analogy_words_all.ix[:10000,]\n",
    "analogy_2 = analogy_words_all.ix[10000+1:16000,]\n",
    "analogy_3 = analogy_words_all.ix[16000+1:20000,]\n",
    "analogy_4 = analogy_words_all.ix[20000+1:,]\n",
    "\n",
    "\n",
    "analogies = [analogy_4,analogy_2, analogy_3, analogy_1]\n",
    "for embedding in embeddings:\n",
    "    print(embedding['name'])\n",
    "    words = embedding['words'] \n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "    batch_accs = []\n",
    "    batch_MRRs = []\n",
    "    batch_lengths = []\n",
    "    \n",
    "    #batch because of memory constraints\n",
    "    for analogy_words in analogies:\n",
    "        start =  time.time()\n",
    "\n",
    "        print('Calculating batch, size:', len(analogy_words))\n",
    "\n",
    "        indexes_found = [] # indexes found in the analogy batch\n",
    "        actual_answer_indices = [] # embedding index for the answers of \n",
    "                                    # the analogy questions\n",
    "        to_exclude = [] # exclude words in query for each question          \n",
    "        all_bstars = [] # holds all predicted vectors\n",
    "        #Do the first 20000 words, and then the rest\n",
    "        \n",
    "        \n",
    "        for index, row in analogy_words.iterrows():\n",
    "                if row[0] in words.keys() and row[1] in words.keys() and row[2] in words.keys() and row[3] in words.keys():   \n",
    "                    #only do for a subset:\n",
    "                    indexes_found.append(index)\n",
    "                    a = words[row[0]]\n",
    "                    astar = words[row[1]]\n",
    "                    b = words[row[2]]\n",
    "                    vocab_keys = list(embedding_vocabulary)\n",
    "\n",
    "                    actual_bstar_index = vocab_keys.index(row[3])\n",
    "\n",
    "                    # Compute offset a* - a, add to b\n",
    "                    bstar = b.T + (astar.T - a.T)            \n",
    "                    cosine_sims = []\n",
    "                    cosine_sims_keys = []\n",
    "\n",
    "                    # Now that we computed  bstar find the closest vector to it\n",
    "                    # compute cosine similarity with all vectors\n",
    "                    # obtain the closest               \n",
    "                    to_exclude.append([vocab_keys.index(row[0]),\n",
    "                                      vocab_keys.index(row[1]),\n",
    "                                      vocab_keys.index(row[2])])\n",
    "                    all_bstars.append(bstar)\n",
    "                    actual_answer_indices.append(actual_bstar_index)\n",
    "\n",
    "        # check if it found something\n",
    "        if len(indexes_found)!=0:\n",
    "            all_bstars_np = np.array(all_bstars)\n",
    "\n",
    "            embeddingMRR, acc, answer_indexes, possible_answer_indexes = accuracy_and_MRR(all_bstars_np, \n",
    "                                                    embedding_vectors, \n",
    "                                                   actual_answer_indices, to_exclude)\n",
    "\n",
    "            print(embeddingMRR, acc, len(indexes_found ))\n",
    "            batch_accs.append(acc)\n",
    "            batch_MRRs.append(embeddingMRR)\n",
    "            batch_lengths.append(len(indexes_found))\n",
    "        \n",
    "        \n",
    "        end = time.time()\n",
    "        print(end-start, \"seconds\")\n",
    "\n",
    "        \n",
    "    embedding.update({'MRR':batch_MRRs, \n",
    "                          'analogy_indexes_found': indexes_found,\n",
    "                          'answer_indexes': answer_indexes,\n",
    "                          'actual_answer_indices': actual_answer_indices,\n",
    "                          'possible_answer_indices': possible_answer_indexes,\n",
    "                          'batch_lengths':batch_lengths,\n",
    "                         'acc':batch_accs})\n",
    "\n",
    "end = time.time()\n",
    "print(end-start, \"seconds\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow2 acc 0.621794871795\n",
      "bow5 acc 0.596574190836\n",
      "bow2 mrr 0.700054195194\n",
      "bow5 mrr 0.690255505427\n"
     ]
    }
   ],
   "source": [
    "# Calculates wighted average for acc and mrr for each of the batches\n",
    "for embedding in embeddings:\n",
    "    \n",
    "    amount = np.array(embedding['batch_lengths'])\n",
    "    rate = embedding['acc']\n",
    "    weights = amount/sum(amount)\n",
    "    acc = np.average(rate, weights=weights)\n",
    "    print(embedding['name'],'acc',acc)\n",
    "    \n",
    "    \n",
    "for embedding in embeddings:\n",
    "    \n",
    "    amount = np.array(embedding['batch_lengths'])\n",
    "    rate = embedding['MRR']\n",
    "    weights = amount/sum(amount)\n",
    "    mrr = np.average(rate, weights=weights)\n",
    "    print(embedding['name'],'mrr',mrr)\n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
