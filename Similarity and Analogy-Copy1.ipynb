{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing  # Two samples, with 3 dimensions.\n",
    "\n",
    "\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "embedding_names = ['bow5', 'deps']\n",
    "\n",
    "\n",
    "simlexf = 'Simlex/SimLex-999.txt'\n",
    "\n",
    "#\n",
    "# embeddings: list of dicts holding the data for each embedding\n",
    "# dict: {name - embedding name,\n",
    "#       filename - embedding data filename\n",
    "#       words - dict holding the words  and their vectors\n",
    "#    \n",
    "embeddings = []\n",
    "\n",
    "\n",
    "for name in embedding_names:\n",
    "    # words: dict, holds all words and their vectors for each embedding\n",
    "    words = {}\n",
    "    words_normalized = {}\n",
    "    filename = name + '.words.bz2'\n",
    "    embedding_df = pd.read_table(path + filename , sep=' ', header=None)\n",
    "    for index, row in embedding_df.iterrows():\n",
    "        words[row[0]] = row[1:]\n",
    "    \n",
    "    embeddings.append({'name':name, 'filename': filename, \n",
    "                            'words': words})\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compute the cosine similarity for each word pair in the simlex\n",
    "#\n",
    "\n",
    "\n",
    "import analyze\n",
    "import evaluate\n",
    "import imp\n",
    "\n",
    "evaluate = imp.reload(evaluate)\n",
    "analyze = imp.reload(analyze)\n",
    "\n",
    "    \n",
    "\n",
    "cosine_similarities = []\n",
    "\n",
    "\n",
    "path = './data/'\n",
    "simlexf = 'SimLex-999/SimLex-999.txt'\n",
    "simlex = pd.read_table(path + simlexf)\n",
    "simlex_sim =  np.array(simlex[\"SimLex999\"]).astype(np.float)\n",
    "\n",
    "MEN_f = 'MEN/MEN_dataset_natural_form_full'\n",
    "MEN = pd.read_table(path + MEN_f, header=None, sep=' ')\n",
    "MEN_sim =  np.array(MEN.ix[:,2]).astype(np.float)\n",
    "\n",
    "similarity_evaluation = pd.DataFrame({})\n",
    "\n",
    "\n",
    "\n",
    "word_pairs = simlex\n",
    "\n",
    "spearman = []\n",
    "pearson = []\n",
    "\n",
    "datasets = {'simlex': simlex_sim, 'MEN': MEN_sim}\n",
    "\n",
    "\n",
    "def compute_correlations(similarities, predicted):\n",
    "        \n",
    "    df = pd.DataFrame({'control': similarities, 'predicted': predicted})\n",
    "    return( {'pearson': df.corr(method = 'pearson')['control'][1], \n",
    "            'spearman': df.corr(method = 'spearman')['control'][1]})\n",
    "    \n",
    "\n",
    "# Calculate cosine similarity between each pair of words in the simlex\n",
    "\n",
    "words_pairs = simlex \n",
    "dataset = 'simlex'\n",
    "for embedding in embeddings:\n",
    "    word_vectors = embedding['words']\n",
    "    cosine_similarities = []\n",
    "    cosine_similarities = evaluate.evaluate_similarity(word_pairs, word_vectors)\n",
    "    embedding.update({'cosine_similarities':cosine_similarities})\n",
    "\n",
    "    cors = compute_correlations(simlex_sim, cosine_similarities)\n",
    "    pearson.append(cors['pearson'])\n",
    "    spearman.append(cors['spearman'])\n",
    "    \n",
    "similarity_evaluation = pd.DataFrame({'embedding': embedding_names, \n",
    "                                      'spearman'+simlex: spearman,\n",
    "                                     'pearson': pearson})\n",
    "\n",
    "\n",
    "print(similarity_evaluation)\n",
    "\n",
    "\n",
    "spearman = []\n",
    "pearson = []   \n",
    "\n",
    "word_pairs = MEN\n",
    "\n",
    "for embedding in embeddings:\n",
    "    word_vectors = embedding['words']\n",
    "    cosine_similarities = []\n",
    "    cosine_similarities = evaluate.evaluate_similarity(word_pairs, word_vectors)\n",
    "    embedding.update({'cosine_similarities':cosine_similarities})\n",
    "\n",
    "    cors = compute_correlations(MEN_sim, cosine_similarities)\n",
    "    pearson.append(cors['pearson'])\n",
    "    spearman.append(cors['spearman'])\n",
    "        \n",
    "similarity_evaluation = pd.DataFrame({'embedding': embedding_names, \n",
    "                                      'spearman': spearman,\n",
    "                                     'pearson': pearson})\n",
    "\n",
    "\n",
    "    \n",
    "print(similarity_evaluation)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = np.stack(list(words2.values()))\n",
    "all_keys = np.stack(list(words2.keys()))\n",
    "print(all_words[1])\n",
    "print(all_keys[1])\n",
    "\n",
    "words['scurrying']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_matrix_word_to_vector_mapping(embedding_vocabulary, embedding_vectors, words_dictionary, verbose=None):\n",
    "    i=5000\n",
    "    embedding_vocabulary[i]\n",
    "    words[embedding_vocabulary[i]]\n",
    "    words[embedding_vocabulary[i]] - embedding_vectors[i]\n",
    "    if verbose:\n",
    "        print(words[embedding_vocabulary[i]])\n",
    "        print(embedding_vectors[i])\n",
    "        \n",
    "        \n",
    "test_matrix_word_to_vector_mapping(embedding_vocabulary, embedding_vectors, words,verbose=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(analogy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_words(vectors):\n",
    "    \"\"\"Normalize embeddings matrix row-wise.\n",
    "    Parameters\n",
    "    ----------\n",
    "      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n",
    "    \"\"\"\n",
    "    print(np.linalg.norm(vectors[50]))\n",
    "    print(np.linalg.norm(vectors, axis=1))\n",
    "    vectors = vectors.T / np.linalg.norm(vectors, axis=1)\n",
    "    return vectors.T\n",
    "\n",
    "normalized_matrix = normalize_words(embedding_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (analyze.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/efiathieniti/PycharmProjects/ULL/ULL/analyze.py\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analyze = imp.reload(analyze)\n",
    "\n",
    "words = embeddings[0]['words']\n",
    "\n",
    "embedding_vectors = np.stack(list(words.values()))\n",
    "\n",
    "embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "d=words\n",
    "\n",
    "for k in d:  # Standardize\n",
    "    d[k] = np.array(d[k]).flatten()\n",
    "    vectors=list(d.values())\n",
    "    vocabulary=d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def nearest_neighbors(vectors, word, k=1, exclude=[], metric=\"cosine\"):\n",
    "        \"\"\"\n",
    "        Find nearest neighbor of given word\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "          word: string or vector\n",
    "            Query word or vector.\n",
    "\n",
    "          k: int, default: 1\n",
    "            Number of nearest neighbours to return.\n",
    "\n",
    "          metric: string, default: 'cosine'\n",
    "            Metric to use.\n",
    "\n",
    "          exclude: list, default: []\n",
    "            Words to omit in answer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          n: list\n",
    "            Nearest neighbors.\n",
    "        \"\"\"\n",
    "        if isinstance(word, string_types):\n",
    "            assert word in self, \"Word not found in the vocabulary\"\n",
    "            v = self[word]\n",
    "        else:\n",
    "            v = word\n",
    "\n",
    "        D = pairwise_distances(self.vectors, v.reshape(1, -1), metric=metric)\n",
    "\n",
    "        if isinstance(word, string_types):\n",
    "            D[self.vocabulary.word_id[word]] = D.max()\n",
    "\n",
    "        for w in exclude:\n",
    "            D[self.vocabulary.word_id[w]] = D.max()\n",
    "\n",
    "        return [self.vocabulary.id_word[id] for id in D.argsort(axis=0).flatten()[0:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analogy Task\n",
    "\n",
    "# Compute offset\n",
    "analogy_file = 'questions-words.txt'\n",
    "analogy_words = pd.read_table(path + analogy_file , sep=' ', header=1)\n",
    "analogy_words.head()\n",
    "\n",
    "\n",
    "\n",
    "for index, row in analogy_words.iterrows():\n",
    "        if row[0] in words.keys() and row[1] in words.keys() and row[2] in words.keys():\n",
    "    \n",
    "            a = words[row[0]]\n",
    "            astar = words[row[1]]\n",
    "            b = words[row[2]]\n",
    "            #print(a.T, astar.T, b.T)\n",
    "            # Compute offset a* - a, add to b\n",
    "            bstar = b.T + (astar.T - a.T)            \n",
    "            cosine_sims = []\n",
    "            cosine_sims_keys = []\n",
    "            \n",
    "            # Now that we computed  bstar find the closest vector to it\n",
    "            # compute cosine similarity with all vectors\n",
    "            # obtain the closest \n",
    "\n",
    "            \n",
    "            #bstar_normal = bstar / np.linalg.norm(bstar) # Normalize the new vector first\n",
    "            #bstar_cosine_similarities = np.dot(embedding_vectors, bstar_normal.T)\n",
    "            print(bstar)\n",
    "            for word_key in words:\n",
    "                print(words[word_key])\n",
    "                cosine_sims_keys.append(word_key)\n",
    "                cosine_sims.append(analyze.cosine_similarity(bstar, words[word_key]))\n",
    "            \n",
    "            i_min2 = np.argmin(cosine_sims)\n",
    "            bstar_word_2 = cosine_sims_keys[i_min2]\n",
    "\n",
    "            \n",
    "            #print(bstar_cosine_similarities[1])\n",
    "            i_min = np.argmin(bstar_cosine_similarities)\n",
    "            \n",
    "            # Obtain word that corresponds to this vector\n",
    "            bstar_word = embedding_vocabulary[i_min]\n",
    "\n",
    "            \n",
    "            print(row[0], row[1], row[2], bstar_word_2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #for word in words.values:\n",
    "               \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
