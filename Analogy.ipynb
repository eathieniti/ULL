{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing  # Two samples, with 3 dimensions.\n",
    "import analyze\n",
    "import evaluate\n",
    "import imp\n",
    "import time\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy \n",
    "\n",
    "path = './data/'\n",
    "\n",
    "embedding_names = ['bow2','bow5', 'deps']\n",
    "embedding_names = ['bow2', 'deps']\n",
    "\n",
    "simlexf = 'Simlex/SimLex-999.txt'\n",
    "\n",
    "#\n",
    "# embeddings: list of dicts holding the data for each embedding\n",
    "# dict: {name - embedding name,\n",
    "#       filename - embedding data filename\n",
    "#       words - dict holding the words  and their vectors\n",
    "#    \n",
    "embeddings = []\n",
    "\n",
    "\n",
    "for name in embedding_names:\n",
    "    # words: dict, holds all words and their vectors for each embedding\n",
    "    words = {}\n",
    "    words_normalized = {}\n",
    "    filename = name + '.words.bz2'\n",
    "    embedding_df = pd.read_table(path + filename , sep=' ', header=None)\n",
    "    for index, row in embedding_df.iterrows():\n",
    "        words[row[0]] = row[1:]\n",
    "    \n",
    "    \n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "    embeddings.append({'name':name, 'filename': filename, \n",
    "                            'words': words, ' embedding_vectors':  embedding_vectors,\n",
    "                       'embedding_vocabulary': embedding_vocabulary})\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to calculate MRR\n",
    "def accuracy_and_MRR(X, Y, indices, exclude):\n",
    "    # \n",
    "    # X: query vectors matrix (size: A x M)\n",
    "    # Y: embedding vectors matrix (size: Nwords x Mfeatures)\n",
    "    # indices: indices of the real (size: A)\n",
    "    # exclude = list of indices to exclude for each analogy word\n",
    "    # Compute distances between each codeword and each other codeword\n",
    "    distance_matrix = scipy.spatial.distance.cdist(X, Y, metric='cosine')\n",
    "    exclude = np.array(exclude)\n",
    "    m = np.amax(distance_matrix)\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        #print(exclude[:,0].flatten())\n",
    "        distance_matrix[np.arange(X.shape[0]), exclude[:,i].flatten()] =  m\n",
    "    # Rank is the number of distances smaller than the correct distance, a\n",
    "    # specified by the indices arg\n",
    "    \n",
    "    n_le = distance_matrix.T <= distance_matrix[np.arange(X.shape[0]), indices]\n",
    "    answer_indexes = np.argsort(distance_matrix)[:,0]\n",
    "    possible_answer_indexes = np.argsort(distance_matrix)[:,1:4]\n",
    "    matches = np.array(indices - answer_indexes)\n",
    "    acc = sum(matches == 0)/len(matches)\n",
    "    n_lt = distance_matrix.T < distance_matrix[np.arange(X.shape[0]), indices]\n",
    "    \n",
    "    return (np.mean(1./n_le.sum(axis=0)),acc , answer_indexes, possible_answer_indexes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "b=np.array([1,0,10,12,3])\n",
    "sum(b==0)/len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load in the analogy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Athens</th>\n",
       "      <th>Greece</th>\n",
       "      <th>Baghdad</th>\n",
       "      <th>Iraq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Athens  Greece  Baghdad         Iraq\n",
       "0  Athens  Greece  Bangkok     Thailand\n",
       "1  Athens  Greece  Beijing        China\n",
       "2  Athens  Greece   Berlin      Germany\n",
       "3  Athens  Greece     Bern  Switzerland\n",
       "4  Athens  Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analogy Task\n",
    "\n",
    "analogy_file = 'questions-words.txt'\n",
    "analogy_words = pd.read_table(path + analogy_file , sep=' ', header=1)\n",
    "analogy_words.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute analogy word and compare to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869362745098 34\n",
      "0.856582633053 34\n",
      "61.18748211860657 seconds\n"
     ]
    }
   ],
   "source": [
    "start =  time.time()\n",
    "\n",
    "all_bstars = []\n",
    "\n",
    "for embedding in embeddings:\n",
    "    words = embedding['words']\n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "    # warning here I only used the 9000 first words!\n",
    "    # also breaks on first one\n",
    "    indexes_found = []\n",
    "    answers = []\n",
    "    \n",
    "    actual_answer_indices = []\n",
    "    to_exclude = []\n",
    "    all_bstars = []\n",
    "    for index, row in analogy_words.head(8400).iterrows():\n",
    "            if row[0] in words.keys() and row[1] in words.keys() and row[2] in words.keys() and row[3] in words.keys():   \n",
    "                #only do for a subset:\n",
    "                indexes_found.append(index)\n",
    "                a = words[row[0]]\n",
    "                astar = words[row[1]]\n",
    "                b = words[row[2]]\n",
    "                vocab_keys = list(embedding_vocabulary)\n",
    "\n",
    "                actual_bstar_index = vocab_keys.index(row[3])\n",
    "\n",
    "                # Compute offset a* - a, add to b\n",
    "                bstar = b.T + (astar.T - a.T)            \n",
    "                cosine_sims = []\n",
    "                cosine_sims_keys = []\n",
    "                # Now that we computed  bstar find the closest vector to it\n",
    "                # compute cosine similarity with all vectors\n",
    "                # obtain the closest \n",
    "\n",
    "                #exclude = [row[0], row[1], row[2]]\n",
    "\n",
    "              \n",
    "                to_exclude.append([vocab_keys.index(row[0]),\n",
    "                                  vocab_keys.index(row[1]),\n",
    "                                  vocab_keys.index(row[2])])\n",
    "                all_bstars.append(bstar)\n",
    "                actual_answer_indices.append(actual_bstar_index)\n",
    "    \n",
    "    all_bstars_np = np.array(all_bstars)\n",
    "\n",
    "    embeddingMRR, acc, answer_indexes, possible_answer_indexes = accuracy_and_MRR(all_bstars_np, \n",
    "                                            embedding_vectors, \n",
    "                                           actual_answer_indices, to_exclude)\n",
    "    \n",
    "    print(embeddingMRR,len(indexes_found ))\n",
    "    embedding.update({'MRR':embeddingMRR, \n",
    "                      'analogy_indexes_found': indexes_found,\n",
    "                      'answer_indexes': answer_indexes,\n",
    "                      'actual_answer_indices': actual_answer_indices,\n",
    "                      'possible_answer_indices': possible_answer_indexes,\n",
    "                     'acc':acc})\n",
    "                      \n",
    "end = time.time()\n",
    "print(end-start, \"seconds\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the results qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow2\n",
      "QUERY: ['boy' 'girl' 'stepson' 'stepdaughter']\n",
      "ANSWERS: ['step-daughter' 'niece' 'daughter-in-law']  \n",
      "QUERY: ['brother' 'sister' 'man' 'woman']\n",
      "ANSWERS: ['girl' 'person' 'tomboy']  \n",
      "deps\n",
      "QUERY: ['boy' 'girl' 'stepson' 'stepdaughter']\n",
      "ANSWERS: ['sister-in-law' 'daughter-in-law' 'stepbrother']  \n",
      "QUERY: ['brother' 'sister' 'man' 'woman']\n",
      "ANSWERS: ['girl' 'waif' 'schoolgirl']  \n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    words = embedding['words']\n",
    "    print(embedding['name'])\n",
    "    embedding_vocabulary = embedding['embedding_vocabulary']\n",
    "    #answer_indexes = embedding['answer_indexes']\n",
    "    analogy_indexes_found= embedding['analogy_indexes_found']\n",
    "    actual_answer_indices= embedding['possible_answer_indices']\n",
    "    \n",
    "    #for i in range(len(analogy_indexes_found)): \n",
    "    for i in [20,33]:\n",
    "        print('QUERY:', analogy_words.values[analogy_indexes_found[i]])\n",
    "        print('ANSWERS:',embedding_vocabulary[actual_answer_indices[i]] ,' ')\n",
    "        #print(analogy_words.ix[row.analogy_indexes_found,1:3])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([8366, 8367, 8368, 8369, 8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379, 8380, 8381, 8382, 8383, 8384, 8385, 8386, 8387, 8388, 8389, 8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399],)\n"
     ]
    }
   ],
   "source": [
    "print(analogy_indexes_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Qualitative comparison look at the first 4 words\n",
    "\n",
    "# Compute accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
