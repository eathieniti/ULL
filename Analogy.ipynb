{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing  # Two samples, with 3 dimensions.\n",
    "import analyze\n",
    "import evaluate\n",
    "import imp\n",
    "import time\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "embedding_names = ['bow2','bow5', 'deps']\n",
    "embedding_names = ['bow2', 'deps']\n",
    "\n",
    "simlexf = 'Simlex/SimLex-999.txt'\n",
    "\n",
    "#\n",
    "# embeddings: list of dicts holding the data for each embedding\n",
    "# dict: {name - embedding name,\n",
    "#       filename - embedding data filename\n",
    "#       words - dict holding the words  and their vectors\n",
    "#    \n",
    "embeddings = []\n",
    "\n",
    "\n",
    "for name in embedding_names:\n",
    "    # words: dict, holds all words and their vectors for each embedding\n",
    "    words = {}\n",
    "    words_normalized = {}\n",
    "    filename = name + '.words.bz2'\n",
    "    embedding_df = pd.read_table(path + filename , sep=' ', header=None)\n",
    "    for index, row in embedding_df.iterrows():\n",
    "        words[row[0]] = row[1:]\n",
    "    \n",
    "    \n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "    embeddings.append({'name':name, 'filename': filename, \n",
    "                            'words': words, ' embedding_vectors':  embedding_vectors,\n",
    "                       'embedding_vocabulary': embedding_vocabulary})\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to calculate MRR\n",
    "def MRR(X, Y, indices, exclude):\n",
    "    # \n",
    "    # X: query vectors matrix (size: A x M)\n",
    "    # Y: embedding vectors matrix (size: Nwords x Mfeatures)\n",
    "    # indices: indices of the real (size: A)\n",
    "    # exclude = list of indices to exclude for each analogy word\n",
    "    # Compute distances between each codeword and each other codeword\n",
    "    distance_matrix = scipy.spatial.distance.cdist(X, Y, metric='cosine')\n",
    "    exclude = np.array(exclude)\n",
    "    m = np.amax(distance_matrix)\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        #print(exclude[:,0].flatten())\n",
    "        distance_matrix[np.arange(X.shape[0]), exclude[:,i].flatten()] =  m\n",
    "    # Rank is the number of distances smaller than the correct distance, a\n",
    "    # specified by the indices arg\n",
    "    n_le = distance_matrix.T <= distance_matrix[np.arange(X.shape[0]), indices]\n",
    "    n_lt = distance_matrix.T < distance_matrix[np.arange(X.shape[0]), indices]\n",
    "    return (np.mean(1./n_le.sum(axis=0)),\n",
    "           np.mean(1./(n_lt.sum(axis=0) + 1)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load in the analogy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Athens</th>\n",
       "      <th>Greece</th>\n",
       "      <th>Baghdad</th>\n",
       "      <th>Iraq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Athens  Greece  Baghdad         Iraq\n",
       "0  Athens  Greece  Bangkok     Thailand\n",
       "1  Athens  Greece  Beijing        China\n",
       "2  Athens  Greece   Berlin      Germany\n",
       "3  Athens  Greece     Bern  Switzerland\n",
       "4  Athens  Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analogy Task\n",
    "\n",
    "analogy_file = 'questions-words.txt'\n",
    "analogy_words = pd.read_table(path + analogy_file , sep=' ', header=1)\n",
    "analogy_words.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute analogy word and compare to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.70005419519385836, 0.70005419519385836) 9516\n",
      "(0.69025550542683178, 0.69025550542683178) 9516\n",
      "(0.67642604656615557, 0.67642604656615557) 9516\n",
      "6475.316911935806 seconds\n"
     ]
    }
   ],
   "source": [
    "start =  time.time()\n",
    "\n",
    "all_bstars = []\n",
    "\n",
    "for embedding in embeddings:\n",
    "    words = embedding['words']\n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "    # warning here I only used the 9000 first words!\n",
    "    # also breaks on first one\n",
    "    indexes_found = []\n",
    "    answers = []\n",
    "    \n",
    "    answer_embedding_indexes = []\n",
    "    to_exclude = []\n",
    "    all_bstars = []\n",
    "    for index, row in analogy_words.iterrows():\n",
    "            if row[0] in words.keys() and row[1] in words.keys() and row[2] in words.keys() and row[3] in words.keys():   \n",
    "                #only do for a subset:\n",
    "                indexes_found.append(index)\n",
    "                a = words[row[0]]\n",
    "                astar = words[row[1]]\n",
    "                b = words[row[2]]\n",
    "                vocab_keys = list(embedding_vocabulary)\n",
    "\n",
    "                actual_bstar_index = vocab_keys.index(row[3])\n",
    "\n",
    "                # Compute offset a* - a, add to b\n",
    "                bstar = b.T + (astar.T - a.T)            \n",
    "                cosine_sims = []\n",
    "                cosine_sims_keys = []\n",
    "                # Now that we computed  bstar find the closest vector to it\n",
    "                # compute cosine similarity with all vectors\n",
    "                # obtain the closest \n",
    "\n",
    "                #exclude = [row[0], row[1], row[2]]\n",
    "\n",
    "              \n",
    "                to_exclude.append([vocab_keys.index(row[0]),\n",
    "                                  vocab_keys.index(row[1]),\n",
    "                                  vocab_keys.index(row[2])])\n",
    "                all_bstars.append(bstar)\n",
    "                answer_embedding_indexes.append(actual_bstar_index)\n",
    "    \n",
    "    all_bstars_np = np.array(all_bstars)\n",
    "\n",
    "\n",
    "\n",
    "    embeddingMRR = MRR(all_bstars_np, embedding_vectors, \n",
    "                       answer_embedding_indexes, to_exclude)\n",
    "    \n",
    "    print(embeddingMRR,len(indexes_found ))\n",
    "    embedding.update({'MRR':embeddingMRR, \n",
    "                      'analogy_words_found': len(indexes_found)})\n",
    "                      \n",
    "end = time.time()\n",
    "print(end-start, \"seconds\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Qualitative comparison look at the first 4 words\n",
    "\n",
    "# Compute accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
