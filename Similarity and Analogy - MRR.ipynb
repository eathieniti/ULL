{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing  # Two samples, with 3 dimensions.\n",
    "\n",
    "\n",
    "\n",
    "path = './data/'\n",
    "\n",
    "embedding_names = ['bow5', 'bow2']\n",
    "embedding_names = ['bow5', 'deps']\n",
    "\n",
    "\n",
    "simlexf = 'Simlex/SimLex-999.txt'\n",
    "\n",
    "#\n",
    "# embeddings: list of dicts holding the data for each embedding\n",
    "# dict: {name - embedding name,\n",
    "#       filename - embedding data filename\n",
    "#       words - dict holding the words  and their vectors\n",
    "#    \n",
    "embeddings = []\n",
    "\n",
    "\n",
    "for name in embedding_names:\n",
    "    # words: dict, holds all words and their vectors for each embedding\n",
    "    words = {}\n",
    "    words_normalized = {}\n",
    "    filename = name + '.words.bz2'\n",
    "    embedding_df = pd.read_table(path + filename , sep=' ', header=None)\n",
    "    for index, row in embedding_df.iterrows():\n",
    "        words[row[0]] = row[1:]\n",
    "    \n",
    "    \n",
    "    embedding_vectors = np.stack(list(words.values()))\n",
    "    embedding_vocabulary = np.stack(list(words.keys()))\n",
    "    embeddings.append({'name':name, 'filename': filename, \n",
    "                            'words': words, ' embedding_vectors':  embedding_vectors,\n",
    "                       'embedding_vocabulary': embedding_vocabulary})\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  embedding   pearson  spearman\n",
      "0      bow5  0.375601  0.367396\n",
      "1      bow2  0.428459  0.414146\n",
      "  embedding   pearson  spearman\n",
      "0      bow5  0.708236  0.723169\n",
      "1      bow2  0.677698  0.699905\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Compute the cosine similarity for each word pair in the simlex\n",
    "#\n",
    "\n",
    "\n",
    "import analyze\n",
    "import evaluate\n",
    "import imp\n",
    "\n",
    "evaluate = imp.reload(evaluate)\n",
    "analyze = imp.reload(analyze)\n",
    "\n",
    "    \n",
    "\n",
    "cosine_similarities = []\n",
    "\n",
    "\n",
    "path = './data/'\n",
    "simlexf = 'SimLex-999/SimLex-999.txt'\n",
    "simlex = pd.read_table(path + simlexf)\n",
    "simlex_sim =  np.array(simlex[\"SimLex999\"]).astype(np.float)\n",
    "\n",
    "MEN_f = 'MEN/MEN_dataset_natural_form_full'\n",
    "MEN = pd.read_table(path + MEN_f, header=None, sep=' ')\n",
    "MEN_sim =  np.array(MEN.ix[:,2]).astype(np.float)\n",
    "\n",
    "similarity_evaluation = pd.DataFrame({})\n",
    "\n",
    "\n",
    "\n",
    "word_pairs = simlex\n",
    "\n",
    "spearman = []\n",
    "pearson = []\n",
    "\n",
    "datasets = {'simlex': simlex_sim, 'MEN': MEN_sim}\n",
    "\n",
    "\n",
    "def compute_correlations(similarities, predicted):\n",
    "        \n",
    "    df = pd.DataFrame({'control': similarities, 'predicted': predicted})\n",
    "    return( {'pearson': df.corr(method = 'pearson')['control'][1], \n",
    "            'spearman': df.corr(method = 'spearman')['control'][1]})\n",
    "    \n",
    "\n",
    "# Calculate cosine similarity between each pair of words in the simlex\n",
    "\n",
    "words_pairs = simlex \n",
    "dataset = 'simlex'\n",
    "for embedding in embeddings:\n",
    "    word_vectors = embedding['words']\n",
    "    cosine_similarities = []\n",
    "    cosine_similarities = evaluate.evaluate_similarity(word_pairs, word_vectors)\n",
    "    embedding.update({'cosine_similarities':cosine_similarities})\n",
    "\n",
    "    cors = compute_correlations(simlex_sim, cosine_similarities)\n",
    "    pearson.append(cors['pearson'])\n",
    "    spearman.append(cors['spearman'])\n",
    "    \n",
    "similarity_evaluation = pd.DataFrame({'embedding': embedding_names, \n",
    "                                      'spearman': spearman,\n",
    "                                     'pearson': pearson})\n",
    "\n",
    "\n",
    "print(similarity_evaluation)\n",
    "\n",
    "\n",
    "spearman = []\n",
    "pearson = []   \n",
    "\n",
    "word_pairs = MEN\n",
    "\n",
    "for embedding in embeddings:\n",
    "    word_vectors = embedding['words']\n",
    "    cosine_similarities = []\n",
    "    cosine_similarities = evaluate.evaluate_similarity(word_pairs, word_vectors)\n",
    "    embedding.update({'cosine_similarities':cosine_similarities})\n",
    "\n",
    "    cors = compute_correlations(MEN_sim, cosine_similarities)\n",
    "    pearson.append(cors['pearson'])\n",
    "    spearman.append(cors['spearman'])\n",
    "    \n",
    "        \n",
    "similarity_evaluation = pd.DataFrame({'embedding': embedding_names, \n",
    "                                      'spearman': spearman,\n",
    "                                     'pearson': pearson})\n",
    "\n",
    "\n",
    "    \n",
    "print(similarity_evaluation)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embedding_vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42c4a7405cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     most_similar = closest_words(embedding['embedding_vectors'],\n\u001b[0m\u001b[1;32m     12\u001b[0m                                  embedding['embedding_vocabulary'], words[word_to_check], 6, exclude=[])\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding_vectors'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Qualitative checks\n",
    "#\n",
    "#\n",
    "# Look into 5 most similar words\n",
    "word_to_check='old'\n",
    "#word_to_check='smart'\n",
    "\n",
    "for embedding in embeddings:\n",
    "    words = embedding['words']\n",
    "    most_similar = closest_words(embedding['embedding_vectors'],\n",
    "                                 embedding['embedding_vocabulary'], words[word_to_check], 6, exclude=[])\n",
    "    \n",
    "    print(most_similar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = np.stack(list(words2.values()))\n",
    "all_keys = np.stack(list(words2.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_words(vectors):\n",
    "    \"\"\"Normalize embeddings matrix row-wise.\n",
    "    Parameters\n",
    "    ----------\n",
    "      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n",
    "    \"\"\"\n",
    "    print(np.linalg.norm(vectors[50]))\n",
    "    print(np.linalg.norm(vectors, axis=1))\n",
    "    vectors = vectors.T / np.linalg.norm(vectors, axis=1)\n",
    "    return vectors.T\n",
    "\n",
    "normalized_matrix = normalize_words(embedding_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import analyze\n",
    "import evaluate\n",
    "import imp\n",
    "\n",
    "analyze = imp.reload(analyze)\n",
    "\n",
    "words = embeddings[0]['words']\n",
    "embedding_vectors = np.stack(list(words.values()))\n",
    "embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALOGY TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def closest_words(embedding_vectors,embedding_vocabulary, word, k, exclude=[]):\n",
    "    #\n",
    "    # Obtain the k most similar words based on cosine similarity\n",
    "    #\n",
    "    D = pairwise_distances(embedding_vectors, word.reshape(1, -1), metric='cosine')\n",
    "    possible_answers=[]\n",
    "    kwords=k\n",
    "    if exclude:\n",
    "        kwords=k+len(exclude)\n",
    "    for id in D.argsort(axis=0).flatten()[0:kwords]:\n",
    "        \n",
    "        # Exclude words in query\n",
    "        if embedding_vocabulary[id] not in (exclude):\n",
    "            possible_answers.append(embedding_vocabulary[id])\n",
    "    return(possible_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogy Task\n",
    "\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Compute offset\n",
    "analogy_file = 'questions-words.txt'\n",
    "analogy_words = pd.read_table(path + analogy_file , sep=' ', header=1)\n",
    "analogy_words.head()\n",
    "\n",
    "\n",
    "for embedding in embeddings:\n",
    "    if embedding['name'] == 'deps':\n",
    "        words = embeddings[0]['words']\n",
    "        embedding_vectors = np.stack(list(words.values()))\n",
    "        embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "        # warning here I only used the 9000 first words!\n",
    "        # also breaks on first one\n",
    "        indexes_found = []\n",
    "        answers = []\n",
    "        for index, row in analogy_words.iterrows():\n",
    "\n",
    "                if row[0] in words.keys() and row[1] in words.keys() and row[2] in words.keys():\n",
    "\n",
    "                    a = words[row[0]]\n",
    "                    astar = words[row[1]]\n",
    "                    b = words[row[2]]\n",
    "\n",
    "                    # Compute offset a* - a, add to b\n",
    "                    bstar = b.T + (astar.T - a.T)            \n",
    "                    cosine_sims = []\n",
    "                    cosine_sims_keys = []\n",
    "                    # Now that we computed  bstar find the closest vector to it\n",
    "                    # compute cosine similarity with all vectors\n",
    "                    # obtain the closest \n",
    "\n",
    "                    exclude = [row[0], row[1], row[2]]\n",
    "\n",
    "                    possible_answers = closest_words(embedding_vectors,embedding_vocabulary, \n",
    "                                                     bstar, 6, exclude)\n",
    "\n",
    "\n",
    "                    answer = possible_answers[0]\n",
    "                    indexes_found.append(index)\n",
    "                    answers.append(possible_answers[1:k])\n",
    "\n",
    "        embedding.update({'analogy_answers':possible_answers, \n",
    "                          'analogy_indexes':indexes_found})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#        with open('answers_' + embedding['name'] + '.csv','w') as f:\n",
    "#           for r in answers:\n",
    "#                f.write(str(r)+'\\n')\n",
    "\n",
    "                      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just save the found indices\n",
    "for embedding in embeddings:\n",
    "    if embedding['name'] == 'deps':\n",
    "        words = embeddings[0]['words']\n",
    "        embedding_vectors = np.stack(list(words.values()))\n",
    "        embedding_vocabulary = np.stack(list(words.keys()))\n",
    "\n",
    "        # warning here I only used the 9000 first words!\n",
    "        # also breaks on first one\n",
    "        indexes_found = []\n",
    "        answers = []\n",
    "        for index, row in analogy_words.iterrows():\n",
    "                if row[0] in words.keys() and row[1] in words.keys() and row[2] in words.keys():\n",
    "                    indexes_found.append(index)\n",
    " \n",
    "\n",
    "    with open('answers_' + embedding['name'] + 'words_found.csv','w') as f:\n",
    "        for r in indexes_found:\n",
    "            f.write(str(r)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('answers_' + embedding['name'] + 'words_found.csv','w') as f:\n",
    "    for r in indexes_found:\n",
    "        f.write(str(r)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('answers_' + embedding['name'] + '.csv','r') as f:\n",
    "    for line in f.readlines():\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
